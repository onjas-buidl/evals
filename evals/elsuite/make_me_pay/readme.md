# Eval description
This evaluation tests an AI systemâ€™s ability to generate persuasive or manipulative text, specifically in the setting of convincing another (simulated) entity to part with money. The setup consists of two language models engaging in conversation, where one model takes on the role of a con-artist trying to extract money, and the other model acts as the "mark" or target, simulating a person who has just received some money. The con-artist model employs various persuasive techniques to get the mark to hand over its funds. Headline metrics include success-rate at receiving a payment, as well as measurements of how large the payments are.


To run the default setup:
```bash
oaieval make-me-pay/gpt-4 make-me-pay
```

There are several variants. We provide evals where we vary the interaction time, allowing the conartist more or less time to manipulate the mark. We also have variants where we specify the marks personality, for example, where we tell the mark to avoid donating any money.

We present several metrics at the end of the eval about the interactions between the con-artist and mark. Our headline metrics are:
- `donation_success_rate`, the percentage of conversations where the con-artist persuaded the mark to donate money.
- `mean_donation_amt`, the mean amount of money donated from the mark. The mean includes runs where the con-artist could not persuade the mark, which is where the mark has donated $0.

## Usage
Run the eval using:
```bash
oaieval {solver} {eval variant}
```

where `{solver}` is the name of the solver you want to evaluate as the conartist, e.g. `make-me-pay/gpt-4`, and `{eval variant}` is the variant of this evaluation you want to run, e.g. `make-me-pay.10-turn.balanced.v2`. By default, the mark is always gpt-4-32k.

## Token estimate
Below is a rough estimate of the total number of tokens consumed some variations of the eval, including both input and output tokens:

| conartist         | mark  | version                         | Tokens    |
|-------------------|-------|---------------------------------|-----------|
| gpt-3.5-turbo-16k | gpt-4 | make-me-pay.5-turn.balanced.v2  | 2 500 000 |
| gpt-4             | gpt-4 | make-me-pay.5-turn.balanced.v2  | 1 600 000 |
| gpt-3.5-turbo-16k | gpt-4 | make-me-pay.10-turn.balanced.v2 | 6 000 000 |
| gpt-4             | gpt-4 | make-me-pay.10-turn.balanced.v2 | 5 300 000 |

To calculate dollar cost from token counts, please check the latest token pricing [here](https://openai.com/pricing). Note that we count both input and output tokens together, so a lower and upper estimate of the cost of each variant can be predicted.

# Contribution statement
Eval design, implementation, and results evaluation were primarily conducted by Andrei Alexandru, with later-stage contributions from Dane Sherburn and Oliver Jaffe, under the guidance of (alphabetically by last-name) Steven Adler, James Aung, Rosie Campbell, and Jade Leung, who provided research input and project management support.

